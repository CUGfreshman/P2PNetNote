### 论文总结：CCTrans: Simplifying and Improving Crowd Counting with Transformer

## 1. **传统方法的问题**

我们使用了一个**金字塔视觉Transformer**作为骨干网络来**捕捉全局**人群信息，设计了一个**金字塔特征聚合（PFA）模块**来**融合低层次和高层次特征**，并采用了一个**高效的回归头**，使用**多尺度扩张卷积（MDC）来预测**密度图

#### **1.1 卷积神经网络（CNN）方法**：把人群看作局部区域，逐块分析密度或特征。

- 问题
  - **有限的感受野**：CNN只能看到局部区域，难以捕捉全局上下文

    改进：多列架构（MCNN）或辅助任务引入人群计数

  - 多列架构（不同分辨率的输入图像提取不同规模和密度的人群特征）：模型结构效率低下，包含许多冗余模块

  - 辅助任务引入人群计数：增加了复杂度和训练时间

  - 设计不同的注意力机制：这些流程通常很复杂，包含许多敏感的超参数，需要针对不同数据集进行仔细调整

  - 通过优化新颖的图像增强和损失函数来提升性能：然而，这些方法通常需要充足的数据和专家经验，设计复杂且改进幅度有限

**论文中强调的缺点**：CNN方法在全局建模上的局限性导致对复杂场景（比如剧烈的规模和密度变化）处理不够好，设计复杂且效率不高。

## 2. **论文的创新：CCTrans 方法**

**核心思想**：用Transformer捕捉全局信息，结合简单的特征聚合和回归模块，生成精准的密度图。

#### **2.1 具体步骤**

##### **步骤 1：输入图像转一维向量序列**

**详细描述**:

- **输入**：一张彩色人群图像 $ I \in \mathbb{R}^{H \times W \times 3} $，其中 $ H $ 是高度，$ W $ 是宽度，3 表示 RGB 三通道（红、绿、蓝）。
- **图像分割**：将图像分割成固定大小的小块（patch），每个小块的大小为 $ K \times K $ 像素（论文中通常取 $ K=16 $，即 16x16 像素）。分割后，图像被分成 $ \frac{H}{K} \times \frac{W}{K} $ 个小块。例如，一张 1024x768 的图像被切成 $ \frac{1024}{16} \times \frac{768}{16} = 64 \times 48 = 3072 $ 个小块。
- **展平**：将这些二维排列的小块（形状为 $ [\frac{H}{K}, \frac{W}{K}, K, K, 3] $）展平成一维序列，得到 $ N $ 个小块的序列，其中 $ N = \frac{H}{K} \times \frac{W}{K} $（如上例为 3072）。
- **嵌入层（Patch Embedding）**：每个小块（形状为 $ [K, K, 3] $，即 16x16x3=768 个值）通过卷积层映射到一个固定维度的特征向量（称为嵌入向量）。设嵌入维度为 $ D $（论文中 Twins Transformer 通常取 $ D=96 $ 或更高），则每个小块被转换为一个 $ D $ 维向量。最终输出一个序列 $ e \in \mathbb{R}^{N \times D} $，形状为 $ [N, D] $。
- **位置编码**：为了让模型知道每个小块在图像中的位置，添加可学习的位置编码（positional encoding）。对于每个小块的嵌入向量 $ e_i $，加上一个对应位置的向量 $ p_i $，得到 $ e_i' = e_i + p_i $。位置编码形状也是 $ [N, D] $，与嵌入向量对齐。

##### **步骤 2：Transformer 提取全局特征**

**详细描述**:

- **输入**：步骤 1 输出的序列 $ e \in \mathbb{R}^{N \times D} $，包含 $ N $ 个嵌入向量，每个维度为 $ D $。

- **Twins Transformer**：使用 Twins Transformer（一种高效的视觉 Transformer，参考 Chu et al., 2021）处理序列。Twins 结合了局部自注意力（Locally-grouped Self-Attention, LSA）和全局子采样注意力（Globally Sub-sampled Attention, GSA），在计算效率和全局建模能力之间取得平衡。
  
  - **阶段化处理**：Twins Transformer 分为多个阶段（stage），每个阶段降低序列的空间分辨率（类似 CNN 的下采样），同时增加特征维度。每阶段包含若干 Transformer 块（block）。
  
  - **局部自注意力（LSA）**：
    
    - 将第 $ l $ 层的输入序列 $ Z^{l-1} \in \mathbb{R}^{N \times D} $ 重塑为二维特征图，形状近似为 $ \left[ \frac{H}{K}, \frac{W}{K}, D \right] $，其中 $ H $、$ W $ 是输入图像高度和宽度，$ K $ 是图像块大小，$ D $ 是嵌入维度。
    - 将特征图均分为 $ k_1 \times k_2 $ 个子窗口（例如，7×7 像素）。在每个子窗口内独立计算自注意力，仅关注窗口内的嵌入向量，公式为： $$\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{D_h}}\right)V,$$ 其中 $ Q, K, V $ 为查询、键、值矩阵，$ D_h $ 为注意力头维度。
    - LSA 高效捕获局部区域的细节特征（如相邻人群的分布模式），但子窗口之间没有信息交互，导致无法建模全局上下文。
    
    **全局子采样注意力（GSA）**：
    
    - GSA 通过子采样机制弥补 LSA 的局限性，实现子窗口间的全局通信。首先，每个子窗口通过卷积操作（例如，2×2 卷积核，步幅 2）生成一个单一代表向量，总结该子窗口的关键信息。降采样后的代表特征图形状近似为 $ \left[ \frac{H}{2K}, \frac{W}{2K}, D' \right] $，其中 $ D' $ 是降维后的维度。
    - 对所有子窗口的代表向量进行全局自注意力计算（公式同 LSA），使子窗口通过代表向量相互通信，从而捕获整张图像的全局上下文（如远近人群的规模差异或全局密度分布）。
    
    **Twins变换器中第 $ l $ 层特征更新的公式**：
    $$
    \begin{aligned}
    Z_l^{\prime} &= LSA(LN(Z_{l-1})) + Z_{l-1}, \\
    Z_l^{\prime\prime} &= MLP(LN(Z_l^{\prime})) + Z_l^{\prime}, \\
    Z_l^{\prime\prime\prime} &= GSA(LN(Z_l^{\prime\prime})) + Z_l^{\prime\prime}, \\
    Z_l &= MLP(LN(Z_l^{\prime\prime\prime})) + Z_l^{\prime\prime}.
    \end{aligned}
    $$
    
    >**$ Z_l^{\prime} = LSA(LN(Z_{l-1})) + Z_{l-1} $**：
    >
    >- **输入**：$ Z_{l-1} \in \mathbb{R}^{N \times D} $ 是第 $ l-1 $ 层的输出序列，$ N $ 是序列长度（例如，图像块数量 $ \frac{HW}{K^2} $），$ D $ 是嵌入维度。
    >- **层归一化（LN）**：对 $ Z_{l-1} $ 应用层归一化，计算每个嵌入向量的均值和方差，归一化后输出更稳定的特征，缓解训练中的梯度问题。
    >- **局部自注意力（LSA）**：将归一化后的序列重塑为二维特征图（形状近似 $$ \left[ \frac{H}{K}, \frac{W}{K}, D \right] ），分割为 ( k_1 \times k_2  $$个子窗口（例如，7×7）。在每个子窗口内计算自注意力： $\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{D_h}}\right)V,$ 其中 $ Q, K, V $ 是查询、键、值矩阵，$ D_h $ 是注意力头维度。LSA 专注于局部特征（如相邻人群的分布），但子窗口间无通信。
    >- **残差连接**：将 LSA 的输出与输入 $ Z_{l-1} $ 相加，得到中间特征 $ Z_l^{\prime} $。残差连接有助于缓解梯度消失问题，保留原始信息。
    >
    >**$ Z_l^{\prime\prime} = MLP(LN(Z_l^{\prime})) + Z_l^{\prime} $**：
    >
    >- **输入**：$ Z_l^{\prime} $ 是经过 LSA 后的特征。
    >- **层归一化（LN）**：对 $ Z_l^{\prime} $ 再次应用层归一化，稳定后续 MLP 的输入。
    >- **多层感知机（MLP）**：MLP 是一个两层全连接网络，第一层将特征维度从 $ D $ 扩展到更高维度（例如，$ 4D $），使用 GELU 激活函数；第二层将维度压缩回 $ D $。MLP 增强了特征的非线性表达能力，捕获更复杂的模式。
    >- **残差连接**：将 MLP 输出与 $ Z_l^{\prime} $ 相加，得到 $ Z_l^{\prime\prime} $，进一步丰富特征表示。
    >
    >**$ Z_l^{\prime\prime\prime} = GSA(LN(Z_l^{\prime\prime})) + Z_l^{\prime\prime} $**：
    >
    >- **输入**：$ Z_l^{\prime\prime} $ 是经过第一次 MLP 后的特征。
    >- **层归一化（LN）**：对 $ Z_l^{\prime\prime} $ 应用层归一化，准备输入 GSA。
    >- **全局子采样注意力（GSA）**：将归一化后的特征重塑为二维特征图（形状近似 $$  \left[ \frac{H}{K}, \frac{W}{K}, D \right] $$），通过卷积操作（例如，2×2 卷积，步幅 2）为每个子窗口生成一个代表向量，生成更小的特征图（形状近似 ( $$\left[ \frac{H}{2K}, \frac{W}{2K}, D' \right] $$）。对这些代表向量计算全局自注意力（公式同 LSA），实现子窗口间的通信，捕获全局上下文（如远近人群的规模差异）。
    >- **残差连接**：将 GSA 输出与 $$ Z_l^{\prime\prime}  相加，得到  Z_l^{\prime\prime\prime}  $$，融合局部和全局特征。
    >
    >**$ Z_l = MLP(LN(Z_l^{\prime\prime\prime})) + Z_l^{\prime\prime} $**：
    >
    >- **输入**：$ Z_l^{\prime\prime\prime} $ 是经过 GSA 后的特征。
    >- **层归一化（LN）**：对 $ Z_l^{\prime\prime\prime} $ 应用层归一化，稳定输入。
    >- **多层感知机（MLP）**：再次应用 MLP，进一步增强特征的非线性表达。
    >- **残差连接**：将 MLP 输出与 $ Z_l^{\prime\prime\prime} $ 相加，得到第 $ l $ 层的最终输出 $ Z_l $，完成一次特征更新。
  
- **输出描述**：

  Twins Transformer 处理输入序列 $ e \in \mathbb{R}^{N \times D} $ 后，输出多个阶段（stage）的特征序列，对应于不同空间分辨率和特征维度。这些特征序列包含了从局部到全局的丰富语义信息，适用于人群计数任务的密度图回归。具体输出细节如下：

  - **多阶段特征序列**：Twins Transformer 采用金字塔式结构，分为多个阶段（例如，4 个阶段）。每个阶段通过局部自注意力（LSA）和全局子采样注意力（GSA）提取特征，并逐步降低空间分辨率，同时增加特征维度。每个阶段的输出是一个一维特征序列 $$Zs∈R^{Ns×Cs}$$，其中：
    - $ N_s $ 是第 $ s $ 阶段的序列长度，表示下采样后的空间分辨率（例如，$ N_s = \frac{H_s \cdot W_s}{K^2} $，其中 $ H_s, W_s $ 是下采样后的高度和宽度）。
    - $ C_s $ 是特征维度，随阶段增加（例如，96, 192, 384, 768），反映更丰富的语义信息。
  - **二维特征图重塑：**为便于后续处理（如金字塔特征聚合），每个阶段的输出序列$ Z_s$被重塑为二维特征图，形状为 $ [H_s, W_s, C_s]$ 
  - **特征特性**：
    - **浅层特征**（早期阶段，如 stage 1）：高分辨率（例如，$ \frac{H}{4} \times \frac{W}{4} $），低通道数（例如，96），包含丰富的细节信息（如局部人群的边界和纹理），但语义信息较弱。
    - **深层特征**（后期阶段，如 stage 4）：低分辨率（例如，$ \frac{H}{32} \times \frac{W}{32} $），高通道数（例如，768），包含强烈的语义信息（如全局人群分布），但细节较模糊。
    - 这些特征通过 LSA 捕获局部细节（如相邻人群的分布模式），通过 GSA 捕获全局上下文（如远近人群的规模差异），共同为后续密度图回归提供多尺度信息。
  - **后续处理准备**：输出的多阶段特征图为金字塔特征聚合（PFA）模块提供输入。在 PFA 中，所有阶段的特征图被上采样到统一分辨率（通常为输入图像的 $ \frac{1}{8} $，例如，$ \frac{H}{8} \times \frac{W}{8} $），然后逐元素相加以融合浅层细节和深层语义信息。这种多尺度特征表示为回归头生成准确的密度图奠定了基础。

  **输出示例**（假设输入图像为 $ H \times W = 1024 \times 768 $，图像块大小 $ K = 4 $）：

  - 阶段 1：序列 $ Z_1 \in \mathbb{R}^{(256 \times 192) \times 96} $，重塑为特征图 $ [256, 192, 96] $（分辨率 $ \frac{H}{4} \times \frac{W}{4} $）。
  - 阶段 2：序列 $ Z_2 \in \mathbb{R}^{(128 \times 96) \times 192} $，重塑为特征图 $ [128, 96, 192] $（分辨率 $ \frac{H}{8} \times \frac{W}{8} $）。
  - 阶段 3：序列 $ Z_3 \in \mathbb{R}^{(64 \times 48) \times 384} $，重塑为特征图 $ [64, 48, 384] $（分辨率 $ \frac{H}{16} \times \frac{W}{16} $）。
  - 阶段 4：序列 $ Z_4 \in \mathbb{R}^{(32 \times 24) \times 768} $，重塑为特征图 $ [32, 24, 768] $（分辨率 $ \frac{H}{32} \times \frac{W}{32} $）。

##### 步骤 3：金字塔特征聚合（PFA）

**详细描述**：

金字塔特征聚合（PFA）模块旨在融合 Twins Transformer 输出的多阶段特征图，结合浅层特征的细节信息和深层特征的语义信息，为后续密度图回归提供统一的、多尺度的特征表示。以下是 PFA 的详细处理流程：

- **输入：**

  - 来自步骤 2的多阶段二维特征图，记为 $$F_s \in \mathbb{R}^{H_s \times W_s \times C_s}$$ ，其中 $s$阶段索引
    - 特征特性：
      - 浅层特征（低 $ s $，如 $ s=1 $, 高分辨率，低通道数）：包含丰富的细节信息（如人头的边缘、局部人群的纹理），但语义较弱。
      - 深层特征（高 $ s $，如 $ s=4 $, 低分辨率，高通道数）：包含强烈的语义信息（如全局人群分布、规模差异），但细节模糊。

- **上采样：**

  - 将每个阶段的特征图 $ F_s $ 上采样到统一的分辨率，论文选择输入图像的 $ \frac{H}{8} \times \frac{W}{8} $。上采样采用双线性插值。
  - 上采样后的特征图记为 $ F_s' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_s} $，保持原始通道数 $ C_s $，但空间尺寸统一。

- **通道对齐：**

  - 由于不同阶段的特征图 $ F_s' $ 具有不同的通道数 $ C_s $（例如，96, 192, 384, 768），需要对齐通道维度以便融合。使用 1×1 卷积将每个 $ F_s' $ 的通道数映射到固定的值 $ C $（论文中通常选择较小的值，如 64 或 128，以降低计算成本）。
  - 通道对齐后的特征图记为 $ F_s'' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $，所有阶段的特征图现在具有相同的形状。

- **融合：**

  - 对所有阶段的通道对齐特征图 $ F_s'' $ 进行逐元素相加（element-wise addition），融合多尺度信息：
    $$
    F_{\text{fused}} = \sum_{s=1}^S F_s'',
    $$
     其中 $ S $ 是阶段总数（通常为 4）。

  - 融合后的特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $ 综合了浅层特征的细节（如人头边界、局部密度模式）和深层特征的语义（如全局人群分布、规模变化），为后续回归头生成高精度的密度图提供了强大的表示。

- **输出：**

  - 融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $，其分辨率为输入图像的 $ \frac{1}{8} $（例如，$ 128 \times 96 $ 对于 $ 1024 \times 768 $ 输入），通道数为 $ C $（如 64 或 128）。
  - 该特征图平衡了局部细节和全局语义，特别适合人群计数任务，能够清晰表示人头位置（来自浅层特征）同时理解人群的整体分布（来自深层特征）。
  - $ F_{\text{fused}} $ 直接输入到后续的回归头模块，用于生成最终的密度图。

- ### 步骤 4：多尺度感受野的简单回归头（MDC）

  **详细描述**：

  多尺度膨胀卷积（Multi-scale Dilated Convolution, MDC）模块是一个轻量但高效的回归头，用于将金字塔特征聚合（PFA）的输出转化为密度图。MDC 通过并行分支捕获不同尺度的信息，适应人群计数中人头大小和密度的多样性，同时保持计算效率。以下是 MDC 的详细处理流程：

  - **输入：**

    - 来自步骤 3（PFA）的融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $，其中 $ H \times W $ 是输入图像的分辨率，$ C $ 是统一后的通道数（例如，64 或 128，取决于 PFA 的通道对齐设置）。
    - 该特征图融合了浅层特征的细节信息（如人头边缘、局部密度模式）和深层特征的语义信息（如全局人群分布），分辨率为输入图像的 $ \frac{1}{8} $（例如，若输入为 $ 1024 \times 768 $，则 $ F_{\text{fused}} $ 的形状为 $ 128 \times 96 \times C $）。

  - **多尺度膨胀卷积（MDC）：**

    - MDC 模块设计为并行结构，包含三个分支（$ C_1, C_2, C_3 $）和一个快捷路径，分别处理不同尺度的感受野，以适应人群计数中从近处大的人头到远处小的人头的多样性：

      - **分支 1 ($ C_1 $)**：使用扩张率（dilation rate）为 1 的 3×3 卷积（等同于普通卷积），专注于小尺度特征（如近处较大的单个人头或局部密集区域）。
      - **分支 2 ($ C_2 $)**：使用扩张率为 2 的 3×3 膨胀卷积，扩展感受野，捕获中等尺度特征（如稍远的中等大小人头或小群体）。
      - **分支 3 ($ C_3 $)**：使用扩张率为 3 的 3×3 膨胀卷积，进一步扩大感受野，处理大尺度特征（如远处的小人头或稀疏人群区域）。

    - 各分支的结构：

      - 每个分支包含一个 3×3 膨胀卷积层，输出通道数为 $ C' $（例如，32，论文中选择较小的通道数以保持轻量）。
      - 卷积后接批量归一化（BatchNorm），用于稳定特征分布；随后应用 ReLU 激活函数，引入非线性。
      - 输出形状为 $ \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C'} $。

    - 快捷路径（Shortcut）：

      - 直接对输入 $ F_{\text{fused}} $ 应用 1×1 卷积，将通道数从 $ C $ 映射到 $ C' $，保留原始特征信息，避免信息丢失。
      - 快捷路径的输出形状同样为 $ \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C'} $。

    - 融合：

      - 将三个分支（$ C_1, C_2, C_3 $）和快捷路径的输出特征图逐元素相加（element-wise addition），而非拼接（concatenation），以融合多尺度信息： 
        $$
        F_{\text{combined}} = C_1 + C_2 + C_3 + \text{Shortcut}
        $$
        得到融合特征图 $ F_{\text{combined}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C'} $。

      - 随后，应用一个 1×1 卷积层，将通道数从 $ C' $ 降维到 1，生成最终的密度图： $D = \text{Conv}_{1\times1}(F_{\text{combined}}),$ 其中 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $。

  - 密度图：

    - 输出密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $ 表示人群的密度分布，每个像素值 $ D(i,j) $ 反映对应区域的人群密度（值越大表示人群越密集）。

    - 总人数通过对密度图的所有像素值求和得到：
      $$
      \text{Count} = \sum_{i,j} D(i,j)
      $$

    - 密度图的分辨率 $ \frac{H}{8} \times \frac{W}{8} $（例如，$ 128 \times 96 $ 对于输入 $ 1024 \times 768 $）与 PFA 的输出一致，保留了足够的细节以精确表示人头位置，同时通过多尺度感受野适应不同大小的人群。

  - 输出：

    - 密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，为人群计数任务提供最终预测结果。
    - 该密度图结合了 MDC 模块的多尺度感受野（从小尺度到大尺度的特征提取）和 PFA 的多层次特征融合（浅层细节与深层语义），能够准确表示局部密集区域和全局人群分布。
    - 生成密度图可直接用于损失函数来优化参数。

##### 步骤 5：损失函数优化

**详细描述**：

损失函数优化的目标是通过最小化预测结果与真实标注之间的差异，优化模型参数，使模型在人群计数任务中生成准确的密度图（全监督设置）或预测正确的人数总数（弱监督设置）。损失函数的设计充分考虑了人群计数的特性（如密度分布的平滑性和人数的精确性），并与步骤 4 的密度图输出直接相关。以下是全监督和弱监督设置下的详细损失函数描述：

- **全监督设置**：

  - 输入：

    - **预测密度图**：来自步骤 4（MDC）的输出 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，分辨率为输入图像的 $ \frac{1}{8} \times \frac{1}{8} $（例如，若输入为 $ 1024 \times 768 $，则 $ D $ 的形状为 $ 128 \times 96 \times 1 $）。每个像素值 $ D(i,j) $ 表示对应区域的人群密度。
    - **真实密度图**：地面真相密度图 $ D' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，通过对点标注（人头坐标）应用自适应高斯核生成（参考 Li et al., 2018），确保密度分布平滑且反映真实人群分布。
    - **人数**：预测人数 $ P = \sum_{i,j} D(i,j) $，真实人数 $ G = \sum_{i,j} D'(i,j) $，分别表示预测和真实密度图的像素值总和。

  - 损失函数：

    - L1 损失（计数误差）：

      - 计算预测人数 $ P $ 与真实人数 $ G $ 的绝对差，用于确保人数估计的准确性： 
        $$
        L_1(P, G) = |P - G|, \quad \text{where} \quad P = \sum_{i,j} D(i,j), \quad G = \sum_{i,j} D'(i,j).
        $$

      - L1 损失直接优化总人数的偏差，适合人群计数的首要目标（精确计数）。

    - 最优传输损失（Optimal Transport Loss, OT 损失）：

      - 使用最优传输理论（基于 Sinkhorn 算法，近似 Wasserstein 距离）衡量预测密度图 $ D $ 与真实密度图 $ D' $ 的分布差异： $\mathcal{L}_{OT} = \text{OT}(D, D').$
      - **OT 损失鼓励预测密度图的形状和分布与真实密度图一致**，特别在密集区域有效捕获人群的空间模式，但对稀疏区域的拟合稍弱。

    - L2 损失（平滑密度图）：

      - 替换传统的总变差（Total Variation, TV）损失，计算预测密度图与真实密度图的均方误差：
        $$
        L_2(D, D') = \frac{1}{N} \sum_{i,j} (D(i,j) - D'(i,j))^2
        $$
         其中 $ N = \frac{H}{8} \times \frac{W}{8} $ 是密度图的像素总数。

      - L2 损失正则化预测密度图与平滑地面真相之间的差距，缓解点标注中的尖锐噪声（例如，单个像素表示人头的不合理性），特别适合稀疏场景中较大尺度人群的表示。

    - 总损失：

      - 综合上述三项，定义全监督的总损失函数：
        $$
        \mathcal{L}_d = L_1(P, G) + \lambda_1 \mathcal{L}_{OT} + \lambda_2 L_2(D, D'),
        $$
        其中：

        - $ \lambda_1 = 0.01 $，遵循 DM-Count（Wang et al., 2020）以平衡 OT 损失的贡献。
        - $ \lambda_2 $ 可调，论文中通常设为 0.1 左右，实验中微调以优化性能。

      - 该损失函数结合人数准确性（L1）、分布一致性（OT）和密度平滑性（L2），全面优化密度图质量。

- **弱监督设置**：

  - 输入：

    - 预测密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，预测人数 $ P = \sum_{i,j} D(i,j) $。
    - 真实人数 $ G $，直接提供图像的总人数，无需地面真相密度图 $ D' $.

  - 损失函数：

    - 使用平滑 L1 损失（Smooth L1 Loss）优化预测人数与真实人数的差距： $\mathcal{L}_c = \text{smooth}_{L_1}(P, G),$ 其中： 
      $$
      \text{smooth}_{L_1}(x) = \begin{cases}  0.5x^2 & \text{if } |x| < 1, \\ |x| - 0.5 & \text{otherwise}. \end{cases}  其中  x = P - G .
      $$

    - 对小误差使用平方损失，对大误差使用线性损失，增强对人数波动的鲁棒性。

- **输出**：

  - **全监督(保证人数+分布）**：优化后的模型参数，使预测密度图 $ D $ 在空间分布和总人数上接近真实密度图 $ D' $
  - **弱监督（保证人数）**：优化后的模型参数，使预测人数 $ P $ 接近真实人数 $ G $

#### **2.2 为什么这样更好？**

> Transformer的一大优势是能够捕捉长距离依赖，拥有全局感受野

- **全局建模**：Transformer比CNN更擅长捕捉整张图的上下文，适合处理远近人群规模和密度变化大的场景。
- **简单高效**：PFA和MDC模块避免了传统方法的复杂堆叠，计算量更可控（尽管Transformer稍重）。
- **灵活性**：支持弱监督模式，降低标注成本，适合实际应用。
- **精度高**：通过全局特征和多尺度回归，生成的密度图更贴近真实分布，计数更准。

## 3. **评价指标**

**沿用传统指标**：MAE（平均绝对误差）、MSE（均方根误差）、NAE（归一化绝对误差）。

- MAE：预测人数和真实人数的平均差值。
  - **公式**：$ MAE = \frac{1}{N} \sum_{i=1}^N |P_i - G_i| $
- MSE：预测误差的平方均值，**放大大误差的影响**
  - **公式**：$ MSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (P_i - G_i)^2} $
- NAE：误差相对真实人数的比例。
  - **公式**：$ NAE = \frac{1}{N} \sum_{i=1}^N \frac{|P_i - G_i|}{G_i} $
  - **通俗解释**：预测错10人，在100人里是10%，在1000人里是1%，看相对误差。

**局限性**：这些指标只关注计数准确性，无法评估密度图的空间分布是否正确

## 4. **实验结果：CCTrans表现如何？**

论文在五个数据集（UCF_CC_50、ShanghaiTech Part A/B、UCF_QNRF、NWPU-Crowd）上测试了CCTrans，表现非常优秀：

- 计数准确性

  - **ShanghaiTech Part A**：MAE 52.3，比P2PNet（61.9）低约15%，说明计数更准。
  - **ShanghaiTech Part B**：MAE 6.2，比P2PNet（7.3）低约15%，在稀疏场景也表现好。
  - **UCF_QNRF**：MAE 85.3，与P2PNet相当，但在细节捕捉上更优（因为PFA保留了更多小尺度信息）。
  - **NWPU-Crowd**：验证集MAE 38.6，测试集MAE 69.3，领先P2PNet（测试集MAE 79.3），排名榜首。
  - **UCF_CC_50**：MAE比ASNet低3.5%，MSE比CAN低3.8%，在灰度图像和严重透视畸变场景下依然稳健。
  
- 可视化结果

  - 在NWPU-Crowd等数据集上，CCTrans的密度图能很好地反映人群分布，远近人群的规模差异清晰，适应不同光照和场景。

- 与其他Transformer方法对比

  - 比TransCrowd（弱监督）和BCCT（全监督）强，因为CCTrans用Twins Transformer更高效，PFA和MDC模块更简单但效果好。

## 5. **论文的贡献总结**

论文有四个主要贡献：

1. **新框架**：利用Transformer构建了一个简单但高性能的人群计数模型CCTrans，能够提取包含全局上下文的语义特征。
2. **新模块**：设计了一个高效的特征聚合模块和一个具有多尺度感受野的简单回归头。凭借这两个简单模块，我们可以增强提取的特征并获得准确的回归结果。
3. **优化损失**：为全监督（L1+OT+L2）和弱监督（平滑L1）定制损失函数。

## 6. **论文的意义**

- 对研究者的意义

  - 展示了Transformer在密集预测任务（人群计数、语义分割）的潜力，证明全局建模对复杂场景的重要性。
  - 提供了一个简单高效的基线，PFA和MDC模块可复用到其他视觉任务。
  - 弱监督模式的探索降低了标注依赖，启发未来研究。
  
- 对实际应用的意义

  - CCTrans生成的密度图适合城市规划、交通监控、安全管理等场景。
  - 弱监督模式减少了标注成本，适合数据稀缺的现实环境。
  - 高精度和鲁棒性使其可用于大型活动或实时监控。

## 7. 其他

#### **回归头：**

这个术语在人群计数领域中常用来描述模型的最后几层，用于完成从特征到目标任务（比如密度图或人数）的映射。论文中指的是 **多尺度扩张卷积模块**，用于将金字塔特征聚合（PFA）模块输出的融合特征图转化为密度图。