### 论文总结：CCTrans: Simplifying and Improving Crowd Counting with Transformer

## 1. **传统方法的问题**

我们使用了一个**金字塔视觉Transformer**作为骨干网络来**捕捉全局**人群信息，设计了一个**金字塔特征聚合（PFA）模块**来**融合低层次和高层次特征**，并采用了一个**高效的回归头**，使用**多尺度扩张卷积（MDC）来预测**密度图

#### **1.1 卷积神经网络（CNN）方法**：把人群看作局部区域，逐块分析密度或特征。

- 问题
  - **有限的感受野**：CNN只能看到局部区域，难以捕捉全局上下文

    改进：多列架构（MCNN）或辅助任务引入人群计数

  - 多列架构（不同分辨率的输入图像提取不同规模和密度的人群特征）：模型结构效率低下，包含许多冗余模块

  - 辅助任务引入人群计数：增加了复杂度和训练时间

  - 设计不同的注意力机制：这些流程通常很复杂，包含许多敏感的超参数，需要针对不同数据集进行仔细调整

  - 通过优化新颖的图像增强和损失函数来提升性能：然而，这些方法通常需要充足的数据和专家经验，设计复杂且改进幅度有限

**论文中强调的缺点**：CNN方法在全局建模上的局限性导致对复杂场景（比如剧烈的规模和密度变化）处理不够好，设计复杂且效率不高。

## 2. **论文的创新：CCTrans 方法**

**论文强调CCTrans的目标**：用Transformer简化人群计数流程，同时提升计数精度。核心是生成高质量的密度图，捕捉全局上下文，支持全监督和弱监督两种模式。

**核心思想**：用Transformer捕捉全局信息，结合简单的特征聚合和回归模块，生成精准的密度图。

#### **2.1 具体步骤**

##### **步骤 1：输入图像转一维向量序列**

**详细描述**:

- **输入**：一张彩色人群图像 $ I \in \mathbb{R}^{H \times W \times 3} $，其中 $ H $ 是高度，$ W $ 是宽度，3 表示 RGB 三通道（红、绿、蓝）。
- **图像分割**：将图像分割成固定大小的小块（patch），每个小块的大小为 $ K \times K $ 像素（论文中通常取 $ K=16 $，即 16x16 像素）。分割后，图像被分成 $ \frac{H}{K} \times \frac{W}{K} $ 个小块。例如，一张 1024x768 的图像被切成 $ \frac{1024}{16} \times \frac{768}{16} = 64 \times 48 = 3072 $ 个小块。
- **展平**：将这些二维排列的小块（形状为 $ [\frac{H}{K}, \frac{W}{K}, K, K, 3] $）展平成一维序列，得到 $ N $ 个小块的序列，其中 $ N = \frac{H}{K} \times \frac{W}{K} $（如上例为 3072）。
- **嵌入层（Patch Embedding）**：每个小块（形状为 $ [K, K, 3] $，即 16x16x3=768 个值）通过卷积层映射到一个固定维度的特征向量（称为嵌入向量）。设嵌入维度为 $ D $（论文中 Twins Transformer 通常取 $ D=96 $ 或更高），则每个小块被转换为一个 $ D $ 维向量。最终输出一个序列 $ e \in \mathbb{R}^{N \times D} $，形状为 $ [N, D] $。
- **位置编码**：为了让模型知道每个小块在图像中的位置，添加可学习的位置编码（positional encoding）。对于每个小块的嵌入向量 $ e_i $，加上一个对应位置的向量 $ p_i $，得到 $ e_i' = e_i + p_i $。位置编码形状也是 $ [N, D] $，与嵌入向量对齐。

##### **步骤 2：Transformer 提取全局特征**

**详细描述**:

- **输入**：步骤 1 输出的序列 $ e \in \mathbb{R}^{N \times D} $，包含 $ N $ 个嵌入向量，每个维度为 $ D $。
- **Twins Transformer**：使用 Twins Transformer（一种高效的视觉 Transformer，参考 Chu et al., 2021）处理序列。Twins 结合了局部自注意力（Locally-grouped Self-Attention, LSA）和全局子采样注意力（Globally Sub-sampled Attention, GSA），在计算效率和全局建模能力之间取得平衡。
  - **阶段化处理**：Twins Transformer 分为多个阶段（stage），每个阶段降低序列的空间分辨率（类似 CNN 的下采样），同时增加特征维度。每阶段包含若干 Transformer 块（block）。
  - **局部自注意力（LSA）**：
    - 将序列重塑为二维特征图（形状近似 $ [\frac{H}{K}, \frac{W}{K}, D] $），然后分组为多个小窗口（比如 7x7 像素）。
    - 在每个窗口内计算自注意力，只关注窗口内的向量（减少计算量）。自注意力公式为： $\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{D_h}}\right)V$ 其中 $ Q, K, V $ 是查询、键、值矩阵，$ D_h $ 是注意力头的维度。
    - LSA 捕捉局部区域的细节，比如相邻人群的分布。
  - **全局子采样注意力（GSA）**：
    - GSA 将特征图（形状近似 $ [\frac{H}{K}, \frac{W}{K}, D] $）通过卷积降采样（如 2x2 卷积，步幅 2），生成更小的特征图（比如 $ [\frac{H}{2K}, \frac{W}{2K}, D'] $）
    - 在这些代表向量间计算全局自注意力，捕捉整张图像的上下文，比如远近人群的规模差异。
  - **多层感知机（MLP）**：每个 Transformer 块还包含 MLP（两层全连接层，带 GELU 激活），增强特征表达。
  - **层归一化（LayerNorm）**：在 LSA、GSA 和 MLP 前应用 LayerNorm，稳定训练。
- **输出**：Twins Transformer 输出多个阶段的特征序列（对应不同分辨率）。每个阶段的序列被重塑为二维特征图，形状为 $ [H_s, W_s, C_s] $，其中 $ H_s, W_s $ 是下采样的分辨率（比如 $ \frac{H}{8}, \frac{W}{8} $），$ C_s $ 是特征通道数（随阶段增加，如 96, 192, 384）。

##### **步骤 3：金字塔特征聚合（PFA）**

**详细描述**:

- **输入**：Twins Transformer 输出的多阶段二维特征图，记为 $ F_s \in \mathbb{R}^{H_s \times W_s \times C_s} $，其中 $ s $ 是阶段索引（$ s=1,2,3,4 $，对应分辨率 $ \frac{H}{8}, \frac{H}{16}, \frac{H}{32}, \frac{H}{64} $）。浅层特征（低 $ s $，如 $ s=1 $) 包含细节（比如人头边缘），深层特征（高 $ s $，如 $ s=4 $) 包含语义（比如人群分布）。
- **上采样：**
  - 将每个阶段的特征图 $ F_s $ 上采样到统一分辨率（论文选择输入图像的 $ \frac{1}{8} $，即 $ \frac{H}{8} \times \frac{W}{8} $）。上采样用双线性插值（bilinear interpolation）或转置卷积（transposed convolution）。
  - 例如，$ F_2 $（分辨率 $ \frac{H}{16} \times \frac{W}{16} $）上采样 2 倍，$ F_4 $（$ \frac{H}{64} \times \frac{W}{64} $）上采样 8 倍，得到 $ F_s' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_s} $。
- **通道对齐**：不同阶段的特征图通道数 $ C_s $ 不同（比如 96, 192, 384）。用 1x1 卷积将每个 $ F_s' $ 的通道数统一到固定值（比如 $ C=64 $），得到 $ F_s'' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $。
- **融合**：将所有阶段的特征图 $ F_s'' $ 逐元素相加（element-wise addition）： $F_{\text{fused}} = \sum_{s=1}^S F_s''$ 输出融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $。
- **输出**：融合特征图 $ F_{\text{fused}} $，包含浅层细节（边界清晰）和深层语义（全局分布），为后续密度图生成做准备。

##### **步骤 4：多尺度感受野的简单回归头（MDC）**

**详细描述**:

- **输入**：PFA 输出的融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $。
- **多尺度扩张卷积（MDC）：**
  - MDC 包含多个并行分支（论文中用 3 个分支），每个分支处理不同尺度的信息：
    - **分支 1**：扩张率（dilation rate）为 1（普通卷积），捕捉小尺度细节（如近处大的人头）。
    - **分支 2**：扩张率 2，捕捉中等尺度（如稍远的人头）。
    - **分支 3**：扩张率 3，捕捉大尺度（如远处小的人头）。
  - 每个分支结构：
    - 一个 3x3 卷积（带扩张率），输出通道数为 $ C' $（比如 32）。
    - 后接批量归一化（BatchNorm）和 ReLU 激活。
  - **快捷路径（Shortcut）**：直接用 1x1 卷积处理输入 $ F_{\text{fused}} $，保留原始信息，输出通道数为 $ C' $。
  - **融合**：将 3 个分支和快捷路径的输出拼接（叠加）（concatenate），得到形状为 $ [\frac{H}{8}, \frac{W}{8}, 4C'] $。再用 1x1 卷积降维到 1 个通道，输出密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $。
- **密度图**：每个像素值表示该区域的人群密度，值越大表示人越多。最终人数通过像素值求和得到： $\text{Count} = \sum_{i,j} D(i,j)$

##### **步骤 5：损失函数优化**

**详细描述**:

- **目标**：优化模型参数，使预测密度图接近真实标注（全监督）或预测人数接近真实总数（弱监督）。

- **全监督：**

  - **输入**：预测密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，真实密度图 $ D' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，由点标注（人头坐标）通过高斯核生成。

  - **损失函数：**

    - **L1 损失（计数误差）**：

      检查预测人数和真实人数差多少 $L_1(P, G) = |P - G|, \quad P = \sum_{i,j} D(i,j), \quad G = \sum_{i,j} D'(i,j)$ 确保预测人数 $ P $ 接近真实人数 $ G $。

    - **最优传输损失（OT 损失）：**

      检查人群的“分布图”是否吻合

      - 使用 Sinkhorn 算法（参考 Wasserstein 距离），优化预测密度图和真实密度图的分布差异： $\mathcal{L}_{OT} = \text{OT}(D, D')$
      - 鼓励预测密度图的形状（人群分布）与真实一致。

    - **L2 损失（平滑密度图）：**

      - 替换传统总变差（TV）损失，计算预测和真实密度图的均方误差： $L_2(D, D') = \frac{1}{N} \sum_{i,j} (D(i,j) - D'(i,j))^2$
      - 确保密度图平滑，避免标注中的尖锐噪声（比如单个像素的高值）。

    - **总损失**： $\mathcal{L}_d = L_1(P, G) + \lambda_1 \mathcal{L}_{OT} + \lambda_2 L_2(D, D')$ 其中 $ \lambda_1 = 0.01 $，$ \lambda_2 $ 可调（论文中约为 0.1）。

- ##### **步骤 6：训练和推理**

  **详细描述**:

  - **训练**
    - **全监督：**
      - **数据**：输入图像和对应的真实密度图（由点标注生成）。
      - **优化**：用总损失 $ \mathcal{L}_d $（L1 + OT + L2）计算梯度，更新模型参数（Twins Transformer、PFA、MDC 的权重）。
      - **优化器**：通常用 Adam 或 AdamW，学习率 $ \eta = 10^{-4} \sim 10^{-5} $，带余弦退火（cosine annealing）调度。
      - **批次**：每批次处理 $ B $ 张图像（论文中 $ B=8 \sim 16 $，受 GPU 内存限制）。
    - **弱监督：**
      - **数据**：输入图像和总人数标注。
      - **优化**：用平滑 L1 损失 $ \text{smooth}_{L_1} $，直接优化预测人数。
      - **批次**：类似全监督，但因无需密度图，计算量稍低。
    - **数据增强**：随机裁剪、翻转、颜色抖动，增强模型鲁棒性。
    - **训练轮数**：论文在 ShanghaiTech 等数据集上训练 300~500 轮（epoch），视数据集大小而定。
  - **推理**
    - **全监督：**
      - 输入一张图像，依次通过步骤 1~4，输出密度图 $ D $。
      - 人数为 $ \sum_{i,j} D(i,j) $。
    - **弱监督：**
      - 同上，输出密度图或直接预测人数（视模型配置）。
    - **后处理**：密度图可能上采样到原图分辨率（用双线性插值），便于可视化。

#### **2.2 为什么这样更好？**

> Transformer的一大优势是能够捕捉长距离依赖，拥有全局感受野

- **全局建模**：Transformer比CNN更擅长捕捉整张图的上下文，适合处理远近人群规模和密度变化大的场景。
- **简单高效**：PFA和MDC模块避免了传统方法的复杂堆叠，计算量更可控（尽管Transformer稍重）。
- **灵活性**：支持弱监督模式，降低标注成本，适合实际应用。
- **精度高**：通过全局特征和多尺度回归，生成的密度图更贴近真实分布，计数更准。

## 3. **评价指标**

**沿用传统指标**：MAE（平均绝对误差）、MSE（均方根误差）、NAE（归一化绝对误差）。

- MAE：预测人数和真实人数的平均差值。
  - **公式**：$ MAE = \frac{1}{N} \sum_{i=1}^N |P_i - G_i| $
- MSE：预测误差的平方均值，**放大大误差的影响**
  - **公式**：$ MSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (P_i - G_i)^2} $
- NAE：误差相对真实人数的比例。
  - **公式**：$ NAE = \frac{1}{N} \sum_{i=1}^N \frac{|P_i - G_i|}{G_i} $
  - **通俗解释**：预测错10人，在100人里是10%，在1000人里是1%，看相对误差。

**局限性**：这些指标只关注计数准确性，无法评估密度图的空间分布是否正确

## 4. **实验结果：CCTrans表现如何？**

论文在五个数据集（UCF_CC_50、ShanghaiTech Part A/B、UCF_QNRF、NWPU-Crowd）上测试了CCTrans，表现非常优秀：

- 计数准确性

  - **ShanghaiTech Part A**：MAE 52.3，比P2PNet（61.9）低约15%，说明计数更准。
  - **ShanghaiTech Part B**：MAE 6.2，比P2PNet（7.3）低约15%，在稀疏场景也表现好。
  - **UCF_QNRF**：MAE 85.3，与P2PNet相当，但在细节捕捉上更优（因为PFA保留了更多小尺度信息）。
  - **NWPU-Crowd**：验证集MAE 38.6，测试集MAE 69.3，领先P2PNet（测试集MAE 79.3），排名榜首。
  - **UCF_CC_50**：MAE比ASNet低3.5%，MSE比CAN低3.8%，在灰度图像和严重透视畸变场景下依然稳健。
  
- 可视化结果

  - 在NWPU-Crowd等数据集上，CCTrans的密度图能很好地反映人群分布，远近人群的规模差异清晰，适应不同光照和场景。

- 与其他Transformer方法对比

  - 比TransCrowd（弱监督）和BCCT（全监督）强，因为CCTrans用Twins Transformer更高效，PFA和MDC模块更简单但效果好。

## 5. **论文的贡献总结**

论文有四个主要贡献：

1. **新框架**：利用Transformer构建了一个简单但高性能的人群计数模型CCTrans，能够提取包含全局上下文的语义特征。
2. **新模块**：设计了一个高效的特征聚合模块和一个具有多尺度感受野的简单回归头。凭借这两个简单模块，我们可以增强提取的特征并获得准确的回归结果。
3. **优化损失**：为全监督（L1+OT+L2）和弱监督（平滑L1）定制损失函数。

## 6. **论文的意义**

- 对研究者的意义

  - 展示了Transformer在密集预测任务（人群计数、语义分割）的潜力，证明全局建模对复杂场景的重要性。
  - 提供了一个简单高效的基线，PFA和MDC模块可复用到其他视觉任务。
  - 弱监督模式的探索降低了标注依赖，启发未来研究。
  
- 对实际应用的意义

  - CCTrans生成的密度图适合城市规划、交通监控、安全管理等场景。
  - 弱监督模式减少了标注成本，适合数据稀缺的现实环境。
  - 高精度和鲁棒性使其可用于大型活动或实时监控。

## 7. 其他

#### **回归头：**

这个术语在人群计数领域中常用来描述模型的最后几层，用于完成从特征到目标任务（比如密度图或人数）的映射。论文中指的是 **多尺度扩张卷积模块**，用于将金字塔特征聚合（PFA）模块输出的融合特征图转化为密度图。