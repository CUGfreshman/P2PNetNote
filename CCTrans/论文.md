### 论文总结：CCTrans: Simplifying and Improving Crowd Counting with Transformer

## 1. **传统方法的问题**

我们使用了一个**金字塔视觉Transformer**作为骨干网络来**捕捉全局**人群信息，设计了一个**金字塔特征聚合（PFA）模块**来**融合低层次和高层次特征**，并采用了一个**高效的回归头**，使用**多尺度扩张卷积（MDC）来预测**密度图

#### **1.1 卷积神经网络（CNN）方法**：把人群看作局部区域，逐块分析密度或特征。

- 问题
  - **有限的感受野**：CNN只能看到局部区域，难以捕捉全局上下文

    改进：多列架构（MCNN）或辅助任务引入人群计数

  - 多列架构（不同分辨率的输入图像提取不同规模和密度的人群特征）：模型结构效率低下，包含许多冗余模块

  - 辅助任务引入人群计数：增加了复杂度和训练时间

  - 设计不同的注意力机制：这些流程通常很复杂，包含许多敏感的超参数，需要针对不同数据集进行仔细调整

  - 通过优化新颖的图像增强和损失函数来提升性能：然而，这些方法通常需要充足的数据和专家经验，设计复杂且改进幅度有限

**论文中强调的缺点**：CNN方法在全局建模上的局限性导致对复杂场景（比如剧烈的规模和密度变化）处理不够好，设计复杂且效率不高。

## 2. **论文的创新：CCTrans 方法**

**核心思想**：用Transformer捕捉全局信息，结合简单的特征聚合和回归模块，生成精准的密度图。

### **2.1 具体步骤**

![](image\CCtrans结构.png)

#### **步骤 1：输入图像转一维向量序列**

实现使用 Twins-SVT-Large，Patch Embedding 使用 4×4 卷积（步幅为 4），因此 Stage 1 分辨率为 $ \frac{H}{4} \times \frac{W}{4} $

**输入**：彩色人群图像 $ I \in \mathbb{R}^{H \times W \times 3} $，其中 $ H $ 为高度，$ W $ 为宽度，3 表示 RGB 通道。

**图像分割**：使用 Twins-SVT-Large，图像分割为 4×4 像素的 patch（$ K = 4 $），产生 $ \frac{H}{4} \times \frac{W}{4} $ 个 patch。

**展平**：将 patch 展平为一维序列，得到 $ N = \frac{H}{4} \times \frac{W}{4} $ 个 patch。

**嵌入层**：通过 4×4 卷积（步幅 4）将每个 patch（4×4×3=48 值）映射为 $ D = 96 $ 维向量，输出序列 $ e \in \mathbb{R}^{N \times 96} $

**位置编码**：使用条件位置编码（CPE），通过动态计算相对位置偏置融入注意力机制，适应不同输入尺寸。

**输出**：序列 $ e \in \mathbb{R}^{N \times 96} $，为 Twins Transformer 提供输入。

#### **步骤 2：Twins-SVT-Large 提取全局特征**

> **输入**：步骤 1 输出的序列 $ e \in \mathbb{R}^{N \times D} $，其中 $ N = \frac{H}{4} \times \frac{W}{4} $（基于 patch 大小 $ K = 4 $），$ D = 96 $（嵌入维度）

每个 Transformer 块交替应用 LSA、MLP、GSA、MLP，通过残差连接融合特征。第$l$层更新公式为：
$$
\begin{aligned}
Z_l^{\prime} &= \text{LSA}(\text{LN}(Z_{l-1})) + Z_{l-1}, \\
Z_l^{\prime\prime} &= \text{MLP}(\text{LN}(Z_l^{\prime})) + Z_l^{\prime}, \\
Z_l^{\prime\prime\prime} &= \text{GSA}(\text{LN}(Z_l^{\prime\prime})) + Z_l^{\prime\prime}, \\
Z_l &= \text{MLP}(\text{LN}(Z_l^{\prime\prime\prime})) + Z_l^{\prime\prime\prime}.
\end{aligned}
$$


- **局部自注意力（LSA）：**

  - 将输入序列重塑为二维特征图，形状为 $ \left[ \frac{H}{K}, \frac{W}{K}, D \right] $（例如，$ 256 \times 192 \times 96 $）。

  - 分割为固定大小窗口（实现中为 7×7 token，约 28×28 像素），在每个窗口内独立计算自注意力： 
    $$
    \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{D_h}}\right)V
    $$
     其中 $ Q, K, V $ 为查询、键、值矩阵，$ D_h $ 为注意力头维度。

  - LSA 捕获局部细节（如相邻人群分布），但窗口间无信息交互。

- **全局子采样注意力（GSA）：**

  - 通过 2×2 卷积（步幅 2）对键进行子采样，生成低分辨率代表特征图（形状近似 $ \left[ \frac{H}{2K}, \frac{W}{2K}, D' \right] $），但输出分辨率保持 $ \frac{H}{K} \times \frac{W}{K} $。

  - 对**代表向量**计算全局自注意力，实现窗口间通信，捕获全局上下文（如远近人群规模差异）。

    **层归一化（LN）**：稳定特征，缓解梯度问题。

  **MLP**：两层全连接网络（扩展维度后压缩），增强非线性表达。

  **残差连接**：保留原始信息，防止梯度消失。

> **输出描述**： Twins Transformer 输出 4 个阶段的特征图，包含从局部到全局的多尺度信息（从底层到高层，分辨率降低，通道数提升），供后续金字塔特征聚合（PFA）和密度图回归使用

#### 步骤 3：金字塔特征聚合（PFA）

**输入：**

- 来自步骤 2的二维特征图序列，记为 $$F_s \in \mathbb{R}^{H_s \times W_s \times C_s}$$ ，其中 $s$阶段索引

**上采样：**

- 将每个阶段的特征图 $ F_s $ 上采样到统一的分辨率，论文选择输入图像的 $ \frac{H}{8} \times \frac{W}{8} $。上采样采用双线性插值。
- 上采样后的特征图记为 $ F_s' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_s} $，保持原始通道数 $ C_s $，但空间尺寸统一。

**通道对齐：**

- 由于不同阶段的特征图 $ F_s' $ 具有不同的通道数 $ C_s $（例如，96, 192, 384, 768），需要对齐通道维度以便融合。使用 1×1 卷积将每个 $ F_s' $ 的通道数映射到固定的值 $ C $（论文中通常选择较小的值，如 64 或 128，以降低计算成本）。
- 通道对齐后的特征图记为 $ F_s'' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $，所有阶段的特征图现在具有相同的形状。

**融合：**

- 对所有阶段的通道对齐特征图 $ F_s'' $ 进行逐元素相加（element-wise addition），融合多尺度信息：
  $$
  F_{\text{fused}} = \sum_{s=1}^S F_s'',
  $$
   其中 $ S $ 是阶段总数（通常为 4）。
  $$
  F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} 
  $$
  

**输出：**

- 融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $
- $ F_{\text{fused}} $ 直接输入到后续的回归头模块，用于生成最终的密度图

#### 步骤 4：多尺度感受野的简单回归头（MDC）

> **输入：**PFA 的融合特征图 $ F_{\text{fused}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C} $

**多尺度膨胀卷积（MDC）：**

- **MDC 结构**：

  三个并行分支$C_1, C_2, C_3$，每支包含两层卷积：

  - **第一层**：普通卷积（1×1 或 3×3）+ 批量归一化（BN）+ ReLU，增强特征表达。
  - 第二层：
    - $ C_1 $: 3×3 卷积（扩张率 1），小感受野，适合近处大尺度人头。
    - $ C_2 $: 3×3 膨胀卷积（扩张率 2），中等感受野，适合中等尺度人头。
    - $ C_3 $: 3×3 膨胀卷积（扩张率 3，等效 5×5 感受野），大感受野，适合远处小人头。
  - 各分支输出通道数 $ C' $（例如，32），形状为 $ \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C'} $.

- **快捷路径（Shortcut）**：

  - 对 $ F_{\text{fused}} $ 应用 1×1 卷积，将通道数从 $ C $ 映射到 $ 3C' $，输出 $ \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 3C'} $
  
- **融合：**

  - **拼接**：将 $ C_1, C_2, C_3 $ 拼接，得到 $ F_{\text{concat}} \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 3C'} $.

  - **逐元素相加**：$ F_{\text{combined}} = F_{\text{concat}} + \text{Shortcut} $.

  - **降维**：通过 1×1 卷积将通道数从 $ 3C' $ 降至 1，生成密度图： 
    $$
    D = \text{Conv}_{1\times1}(F_{\text{combined}})
    $$
    

>**输出**：
>
>- 密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，像素值 $ D(i,j) $ 表示人群密度。
>- 总人数：$ \text{Count} = \sum_{i,j} D(i,j) $.

#### 步骤 5：损失函数优化

**全监督设置**：

>**输入：**
>
>- 预测密度图 $ D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，预测人数 $ P = \sum_{i,j} D(i,j) $.
>
>- 真实密度图 $ D' \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 1} $，由点标注经自适应高斯核生成（参考 Li et al., 2018）。
>
>- 真实人数 $ G = \sum_{i,j} D'(i,j) $.

- **损失函数**：

  - L1 损失：

    - 计算预测人数 $ P $ 与真实人数 $ G $ 的绝对差，用于确保人数估计的准确性： 
      $$
      L_1(P, G) = |P - G|, \quad \text{where} \quad P = \sum_{i,j} D(i,j), \quad G = \sum_{i,j} D'(i,j).
      $$

    - L1 损失直接优化**总人数的偏差**

  - **最优传输损失（OT Loss）**： 
    $$
    \mathcal{L}_{OT} = \text{OT}(D, D')
    $$
     使用 Sinkhorn 算法近似 Wasserstein 距离，鼓励**密度图分布一致**。

  - L2 损失（平滑密度图）：

    - 替换传统的总变差（Total Variation, TV）损失，计算预测密度图与真实密度图的均方误差：
      $$
      L_2(D, D') = \frac{1}{N} \sum_{i,j} (D(i,j) - D'(i,j))^2
      $$
       其中 $ N = \frac{H}{8} \times \frac{W}{8} $ 是密度图的像素总数。

    - L2 损失正则化预测密度图与平滑地面真相之间的差距，缓解点标注中的尖锐噪声（例如，单个像素表示人头的不合理性），特别适合稀疏场景中较大尺度人群的表示。

  - **总损失**：
    $$
    \mathcal{L}_d = L_1(P, G) + \lambda_1 \mathcal{L}_{OT} + \lambda_2 L_2(D, D')
    $$
    
    
    - $ \lambda_1 = 0.01 $（参考 DM-Count）。
    - $ \lambda_2 = 1 $（主实验默认，表 6 灵敏度分析测试 $ \lambda_2 = 0.1 $）。
    - **数据集特定调整**：对于 ShanghaiTech Part B 和 UCF-QNRF，OT Loss 替换为 Bayesian Loss（附录 Training Setting），以应对尺度剧烈变化。

**弱监督设置**：

>输入：
>
>- 预测密度图 $ D $，预测人数 $ P = \sum_{i,j} D(i,j) $.
>- 真实人数 $ G $（无密度图 $ D' $）。

- 损失函数：

  - 平滑 L1 损失：
    $$
    \mathcal{L}_c = \text{smooth}_{L_1}(P, G), \quad \text{smooth}_{L_1}(x) = \begin{cases}  0.5x^2 & \text{if } |x| < 1, \\ |x| - 0.5 & \text{otherwise} \end{cases}, \quad x = P - G
    $$
    增强对人数波动的鲁棒性。

**输出**：

- **全监督(保证人数+分布）**：优化后的模型参数，使预测密度图 $ D $ 在空间分布和总人数上接近真实密度图 $ D' $
- **弱监督（保证人数）**：优化后的模型参数，使预测人数 $ P $ 接近真实人数 $ G $



### **2.2 为什么这样更好？**

> Transformer的一大优势是能够捕捉长距离依赖，拥有全局感受野

- **全局建模**：Transformer比CNN更擅长捕捉整张图的上下文，适合处理远近人群规模和密度变化大的场景。
- **简单高效**：PFA和MDC模块避免了传统方法的复杂堆叠，计算量更可控（尽管Transformer稍重）。
- **灵活性**：支持弱监督模式，降低标注成本，适合实际应用。
- **精度高**：通过全局特征和多尺度回归，生成的密度图更贴近真实分布，计数更准。



## 3. **评价指标**

**沿用传统指标**：MAE（平均绝对误差）、MSE（均方根误差）、NAE（归一化绝对误差）。

- MAE：预测人数和真实人数的平均差值。
  - **公式**：$ MAE = \frac{1}{N} \sum_{i=1}^N |P_i - G_i| $
- MSE：预测误差的平方均值，**放大大误差的影响**
  - **公式**：$ MSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (P_i - G_i)^2} $
- NAE：误差相对真实人数的比例。
  - **公式**：$ NAE = \frac{1}{N} \sum_{i=1}^N \frac{|P_i - G_i|}{G_i} $
  - **通俗解释**：预测错10人，在100人里是10%，在1000人里是1%，看相对误差。

**局限性**：这些指标只关注计数准确性，无法评估密度图的空间分布是否正确

## 4. **实验结果：CCTrans表现如何？**

论文在五个数据集（UCF_CC_50、ShanghaiTech Part A/B、UCF_QNRF、NWPU-Crowd）上测试了CCTrans，表现非常优秀：

- 计数准确性

  - **ShanghaiTech Part A**：MAE 52.3，比P2PNet（61.9）低约15%，说明计数更准。
  - **ShanghaiTech Part B**：MAE 6.2，比P2PNet（7.3）低约15%，在稀疏场景也表现好。
  - **UCF_QNRF**：MAE 85.3，与P2PNet相当，但在细节捕捉上更优（因为PFA保留了更多小尺度信息）。
  - **NWPU-Crowd**：验证集MAE 38.6，测试集MAE 69.3，领先P2PNet（测试集MAE 79.3），排名榜首。
  - **UCF_CC_50**：MAE比ASNet低3.5%，MSE比CAN低3.8%，在灰度图像和严重透视畸变场景下依然稳健。
  
- 可视化结果

  - 在NWPU-Crowd等数据集上，CCTrans的密度图能很好地反映人群分布，远近人群的规模差异清晰，适应不同光照和场景。

- 与其他Transformer方法对比

  - 比TransCrowd（弱监督）和BCCT（全监督）强，因为CCTrans用Twins Transformer更高效，PFA和MDC模块更简单但效果好。

## 5. **论文的贡献总结**

论文有四个主要贡献：

1. **新框架**：利用Transformer构建了一个简单但高性能的人群计数模型CCTrans，能够提取包含全局上下文的语义特征。
2. **新模块**：设计了一个高效的特征聚合模块和一个具有多尺度感受野的简单回归头。凭借这两个简单模块，我们可以增强提取的特征并获得准确的回归结果。
3. **优化损失**：为全监督（L1+OT+L2）和弱监督（平滑L1）定制损失函数。

## 6. **论文的意义**

- 对研究者的意义

  - 展示了Transformer在密集预测任务（人群计数、语义分割）的潜力，证明全局建模对复杂场景的重要性。
  - 提供了一个简单高效的基线，PFA和MDC模块可复用到其他视觉任务。
  - 弱监督模式的探索降低了标注依赖，启发未来研究。
  
- 对实际应用的意义

  - CCTrans生成的密度图适合城市规划、交通监控、安全管理等场景。
  - 弱监督模式减少了标注成本，适合数据稀缺的现实环境。
  - 高精度和鲁棒性使其可用于大型活动或实时监控。
